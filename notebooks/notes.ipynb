{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL agent \n",
    "A RL agent executes a sequence of actions and observe states and rewards, with major components of value function, policy and model.\n",
    "\n",
    "### RL problem\n",
    "A RL problem may be formulated as a prediction, control or\n",
    "planning problem.\n",
    "* prediction\n",
    "    * The prediction problem, or policy evaluation, is to compute the state or state-action value function for a policy.\n",
    "        * TD learning algorithms are fundamental for evaluating/predicting value functions.\n",
    "            * SARSA (Sutton and Barto, 2017) \n",
    "            * Q-learning (Watkins and Dayan, 1992)\n",
    "* control\n",
    "    * Control algorithms find optimal policies.\n",
    "* planning\n",
    "    * We use the term `planning` to refer to any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment.\n",
    "\n",
    "### Large scale problem\n",
    "Turning the infeasible dynamic programming methods into practical algorithms so that they can be applied to large-scale problems.\n",
    "* use samples to compactly represent the dynamics of the control problem.\n",
    "    * it allows one to deal with learning scenarios when the dynamics is unknown. \n",
    "    * even if the dynamics is available, exact reasoning that uses it might be intractable on its own.\n",
    "* use powerful function approximation methods to compactly represent value functions.\n",
    "\n",
    "### RL algorithms\n",
    "* value VS policy\n",
    "    * value based: \n",
    "        * state value\n",
    "        * state-action value\n",
    "    * policy based\n",
    "* model-free VS ,model-based:\n",
    "    * model-free: the model (state transition function) is not known or learned from experience.\n",
    "    * When the system model is available, we use dynamic programming, which transforms the problem of finding a good controller into the problem of finding a good value function.\n",
    "    * When there is no model, we resort to RL methods. RL methods also work when the model is available.\n",
    "* policy:\n",
    "    * The notion of on-policy and off-policy can be understood as same-policy and different-policy.\n",
    "    * on-policy: evaluate or improve the behavioural policy. \n",
    "        * SARSA evaluates the policy based on samples from the same policy, then refines the policy greedily with respect to action values.\n",
    "    * off-policy: learns an optimal value function/policy, maybe following an unrelated behavioural policy\n",
    "        * the policy Q-learning obtains is usually different from the policy that generates the samples.\n",
    "* function approximation\n",
    "    * with:\n",
    "    * without:\n",
    "* backups\n",
    "    * sample backups (TD and Monte Carlo) \n",
    "        * one-step return (TD(0) and dynamic programming) \n",
    "        * multi-step return (TD(λ), Monte Carlo, and exhaustive search)\n",
    "    * full backups (dynamic programming and exhaustive search)\n",
    "* online learning VS batch learning\n",
    "    * online learning: training algorithms are executed on data acquired in sequence.\n",
    "    * batch learning: models are trained on the entire data set.\n",
    "\n",
    "### When combining off-policy, function approximation, and bootstrapping, we face instability and divergence.\n",
    "\n",
    "* function approximation for scalability and generalization, \n",
    "    * Linear function approximation: Gradient-TD (Sutton et al., 2009a;b; Mahmood et al., 2014), Emphatic-TD (Sutton et al., 2016) and Du et al. (2017)\n",
    "    * Non-linear function approximation: Deep Q-Network (Mnih et al., 2015) and AlphaGo (Silver et al., 2016a)\n",
    "* bootstrapping for computational and data efficiency,\n",
    "    * bootstrapping, an estimate of state or action value is updated from subsequent estimates.\n",
    "* off-policy learning for freeing behaviour policy from target policy.\n",
    "    \n",
    "### Other\n",
    "* Sup (\"supremum\") means, basically, the largest.\n",
    "* Lipschitz function, a function f such that\n",
    "\\begin{equation}\n",
    " |f(x)-f(y)|<=C|x-y| \n",
    "\\end{equation}\n",
    "for all x and y, where C is a constant independent of x and y, is called a Lipschitz function. For example, any function with a bounded first derivative must be Lipschitz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asynchronous advantage actor-critic (A3C)\n",
    "* learns both a policy and a state-value function\n",
    "    * the value function is used for bootstrapping, i.e., updating a state from subsequent estimates, to reduce variance and accelerate learning.\n",
    "* parallel actors employ different exploration policies to stabilize training, so that experience replay is not utilized.\n",
    "* Experience replay: build data-set from agent’s experience\n",
    "* Critic estimates value of current policy by DQN\n",
    "* Actor updates policy in direction that improves Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
